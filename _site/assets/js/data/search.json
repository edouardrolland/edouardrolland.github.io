[
  
  {
    "title": "MSc Thesis - Presentation",
    "url": "/posts/MSc-Thesis-Presentation/",
    "categories": "MSc Thesis",
    "tags": "Academic, Studies, Bristol, Machine Vision, Robotics, Drone, Volcanoes, Plume, trajectory planning, Visual Servoing",
    "date": "2023-06-06 00:00:00 +0200",
    





    
    "snippet": "This article details the presentation and preliminary studies leading up to the completion of my Master’s thesis conducted at the University of Bristol.The University of Bristol has been involved i...",
    "content": "This article details the presentation and preliminary studies leading up to the completion of my Master’s thesis conducted at the University of Bristol.The University of Bristol has been involved in past campaigns collecting volcanic ashes using drones, as showcased in the following video.  Volcanic ash collection thanks to a fixed wing droneAll the different campaigns were conducted at the Fuego Volcano located in Guatemala.Aims and Objectives of this MSc ThesisThis project aims to understand how to collect ashes at a single pointin the smoke plume’s space using machine vision and more particularlyvisual servoing.Core Objectives      Create and label a database of volcano plume images from existingvideo of Fuego volcano in Guatemala.        Train an agent on the database and test on existing plume video.This could be on stills or video.        Create a predictor for the location of the plume taking into accountthe wind direction.        Create and simulate a visual servoing method to bring the aircraftto an appropriate point in the plume.  Stretch Objectives      To design a forebody for BUDDI of less than 800g (Bristol University  Drone Design Initiative) to house the camera, a computing bay and a  sample collector.        To develop code which automatically records the location of the  samples that are taken based on the machine vision and recorded  aircraft tracks/location.  The stretch objectives are designed to allow me to add a hardware dimension to this project. The main objectives only involve software development. With a strong interest in combining hardware and software creation, the CAD (Computer-Aided Design) part will enable me to showcase the skills I have developed in the past.MotivationWhy study volcanic ash plumes ?Volcanic eruptions can release ash, water vapour, and gas into theatmosphere, impacting local populations and air traffic. For instance,the eruption of Eyjafjallajökull in 2010 led to the largest air trafficdisruption in the past decade, with 48% of European air trafficsuspended (Petursdottir &amp; Reichardt, n.d.; Bolić &amp; Sivčev, 2011). Studying the chemicalcomposition and atmospheric dispersion of volcanic ash allows us tomonitor the evolution of eruptions and evaluate potential impacts on airtraffic and the environment (Gouhier, 2019).        Fig.1 Eyjafjallajökull eruption, from https://www.tripsavvy.com/The impact of volcanic eruptions on air traffic is assessed using twomain tools: satellite remote sensing and advection-diffusion dispersionmodels (Stohl et al., 2011). However, to generate reliable forecastsusing these predictive models, measuring the particle size distribution(PSD) of the ash present in the plume is crucial. Although thisparameter can be determined through satellite measurements (Prata, 1989), acquiring data through optical remote sensing canbe challenging under certain weather conditions and introduce latency (Prudente et al., 2020). In-situ sampling within the plume provides amethod for greater operational flexibility, allowing for betteranticipation of the impact of an eruption.Why take ash plume samples with a drone ?As sampling must be done directly inside the smoke plume, it appearsdangerous to use a human-crewed aircraft. Ashes can have catastrophicconsequences on the engines, and ballistic shots near the crater candamage the airframe (Gordeev &amp; Girina, 2014). Collecting smoke plume samples bydrone appears to be the preferred solution. The endurance and range of afixed-wing drone allow these models to perform operations over longdistances. Thus, the team of scientists in charge of the expedition canbe located several kilometres away from the site of the eruption.Bristol University Drone Design Initiative (BUDDI)The Flight Lab at the University of Bristol is actively involved inresearch on using drones in volcanology (Watson, 2017). From 2017to 2019, several expeditions were organized to the Fuego volcano inGuatemala to test various methods of sampling plume ash. All of theseexperiments were carried out using commercial fixed-wing drone platforms (Schellenberg, 2020). In 2018, under the impetus of the CASCADEprogram (Configuration, Analysis, and Design Exploratory), theUniversity of Bristol developed an open-source vertical take-offaircraft design sized to perform volcanic surveillance missions usingthe knowledge gained from experiments around the Fuego volcano (David et al., 2021). The \"Bristol University Drone Design Initiative\"(BUDDI) model is now to be equipped with a payload that will enable itto carry out volcanic surveillance. The latter must enable the drone toperform sample collection in a manner analogous to the previous dronesintroduced in (Schellenberg, 2020), but also integrate the hardwareand software elements necessary to carry out autonomous samplecollection.        Fig.2 Bristol University Drone Design Initiative, illustration from (David et al., 2021)Autonomous coordinated plume interceptionDuring the previous trips to the Fuego, the various drones used wereequipped with an onboard trajectory planning system to optimize flightautonomy up to the summit of the volcano. The collection process was notentirely automated, as the drone’s movement through the smoke plume wascontrolled manually by an operator on the ground(Schellenberg, 2020). Thus, an improvement of the systems developedby the Flight Lab teams would be to automate the control of the droneduring the collection phases. It would reduce pilot workload andoptimize drone trajectory/autonomy in real-time, as human pilotingintroduces inaccuracies and errors. Full automation of the samplingprocess would increase efficiency and repeatability.        Fig.2 Plume interception conducted by the University of Bristol in Guatemala (Schellenberg, 2020)The first step is to develop a robust method to allow the drone todetect the appearance of an ash plume in the atmosphere. In previousFuego excursions, the drones were equipped with high-definition camerasto record the flights. These videos provide a sufficient database totrain a machine-learning model to detect smoke plumes (Flight Lab, n.d.).Nevertheless, the similarity of the clouds surrounding the volcano andthe plumes may challenge training a sufficiently accurate neuralnetwork. However, the literature reports several similar studies:detecting fire smoke using neural networks (Zhang et al., 2020; Frizzi et al., 2017). The results obtained support theidea that this method is feasible for detecting volcanoes ash plumes. Byusing a database containing both clouds and fire smoke, (Zhang et al., 2020)achieved a detection rate of 99.33%. Next, we will focus on implementingthe control law that allows the drone to intercept the plume byconsidering its movement.Installing a camera on the BUDDI will enable a machine-learning model todetect plumes in flight, which can then serve as an input for thedrone’s control law. However, since real testing won’t be feasibleduring this project, developing a numerical simulation to test thevarious control algorithms envisaged becomes imperative. To numericallysimulate an interception, it will be necessary first to understand howto model the movement of an ash plume based on physical laws. The finalpart of this project will be to integrate the necessary components intothe BUDDI fuselage to implement machine vision detection, which will beused in the next test campaigns in Guatemala in 2024.This project is in the continuity of the Flight Lab’s work, particularlyby B. Schellenberg (Schellenberg, 2020). Automating drone samplingwould increase the number of samples taken during a sampling campaign.Still, it would also make it possible to obtain an accurate estimate ofthe sampling area by comparing the data acquired by the sensors. Aprecise knowledge of the sampling area is important, as its distancefrom the crater is a necessary parameter for using ash dispersion models (Stohl et al., 2011).Literature ReviewUse of UAVs for autonomous ash plume samplingThe potential of drones to carry out scientific missions has been knownsince the emergence of this technology. Multi-rotors and fixed-wingmodels have been used since the 2000s to perform Earth science missions (Klemas, 2015; Wegener et al., 2004). The literaturereports on the first use of a drone for volcanic monitoring in 2007 (McGonigle et al., 2008). Since then, multirotors or fixed-wing droneshave been used in numerous volcanic surveillance missions. In 2017,Stefano et al. used a multirotor to take ash samples during an eruptionin Indonesia. The University of Bristol Flight Lab used fixed-wingaircraft to collect ash plume samples from 2017 to 2019 (Di Stefano et al., 2018; Schellenberg, 2020).Several approaches have been used to collect samples directly from theplume of smoke. The first involves manually piloting the drone to thesampling point using line of sight or a First Person View (FPV) system (McGonigle et al., 2008). The second approach involves using anautopilot to follow a pre-defined flight plan and fly over the samplingarea. This method was used to collect ash samples from the plume in 2014at Mount Ontake (Mori et al., 2016). The Flight Lab at the University ofBristol used a hybrid method where only the ascent to the top of thevolcano was autonomous using a real-time trajectory planner. Thesampling was carried out using FPV piloting (Schellenberg, 2020).There is no mention in the literature of autonomous plume interception.However, similar applications to forest fires can be found: Montes etal. present a biomimetic algorithm based on the behaviour of moths totrack smoke plumes and trace them back to the source of the fire. Thisconcept does not apply to our study because the algorithm is based on amulti- copter’s flying behaviour and uses a carbon dioxide sensor (Montes et al., 2014).Modelling the movement of an ash plumeIn light of the elements mentioned in the core objectives, it is necessary to investigate how tomodel the movement of an ash plume to achieve the most realisticsimulation possible. The ash emissions from Fuego are discontinuous. Inthe work previously carried out by the Flight Lab, B. Schellenbergmodelled the movement of volcanic ash plume to optimize the trajectorybetween two collection points. The movement was modelled by consideringthe ash plume as a homogeneous mass of constant volume moving in thedirection and at the speed of the wind. No other physical phenomena weretaken into account (Schellenberg, 2020).The literature mentions more numerical complex models for modellingvolcanic plumes emanating from the crater. Two-dimensional integralmodels are computationally inexpensive and based on the assumption thatgases and emitted particles are in thermal equilibrium with theatmosphere. By establishing mechanical considerations, it is thenpossible to model the system in the form of differential equations (Degruyter &amp; Bonadonna, 2012). Woodhouse et al. were able to model the effects of wind ona volcanic ash plume based on the observation made during the eruptionof Eyjafjallajökull (Woodhouse et al., 2013). The evolution of computing power overthe past decades has made it possible to make three-dimensional modelsmore complex and refined (Cao et al., 2018). Using the 3-dimensional Eulertransport equations, Cerminara et al. were able to model the evolutionof a volcanic plume considered as a mixture of gases and dispersedsolids (Cerminara et al., 2016).Machine Vision for detecting plumesTo use visual detection by a camera of the plume as input for a controllaw to steer the drone, it is necessary to conduct a state-of-the-artreview of associated machine vision methods. The literature highlightsthe use of 2 different types of methods for detecting the presence ofsmoke in an image.Methods based on image analysisSeveral image analysis-based approaches have been proposed for detectingthe presence of smoke in images. The first approach uses colour andstatistical methods for detection (Çelik et al., 2007). Ebert et al.successfully detected smoke by analyzing temporal variations in colourintensity (Qi &amp; Ebert, 2009). A second approach is based on analyzing motionin images. Ho et al. use a motion history detection algorithm coupledwith a fuzzy reasoning system (Ho, 2009). Finally, a thirdapproach involves analyzing textures in the image using matrix analysismethods such as Singular Value Decomposition (SVD) or the Local BinaryPattern method (Chetverikov et al., 2011; Park &amp; Bae, 2020).In the context of our application, the first two methods havelimitations. The characterization of smoke using its colour is not asufficiently robust parameter, as Celik et al. indicated: the smokecolour depends on many parameters (Çelik et al., 2007). Characterization basedon movement requires a fixed camera viewpoint, which will not be presentduring a drone flight. Nevertheless, the method developed by(Chetverikov et al., 2011) is claimed to be robust and insensitive to cameramotion.Methods based on machine learningThe literature of the last 10 years also reports deep learning methodsto detect fires more especially smoke. The most commonly representedapproach in the literature is Convolutional Neural Networks (CNNs). Thistype of network is composed of several convolution layers that filterthe input data to extract important features from the image (Hakim &amp; Brahim, 2018).This type of neural network has demonstrated great performance in objectclassification. As mentioned in the motivation section, they can achieve classification rates of over98% for smoke detection problems.Regarding the different existing architectures, Pundir et al. and Zhangused a dual deep-learning network (Pundir &amp; Raman, 2019; Zhang et al., 2020). Pundir’s neuralnetwork consists of two parts: one to extract data based on smokebehaviour (colour, texture) and another to extract features related tosmoke movement. The features from both parts are combined to achieveclassification. This model trained with 14,000 images, achieves anaccuracy of 98.3% for close-range smoke and 91.96% for distant smoke(Pundir &amp; Raman, 2019). Han et al. used an improved LeNet-5 network model and achievedan accuracy of 94.24% by training their model using a database of 3400images (Han et al., 2022). Zheng et al. evaluate the performance of differentarchitectures in the case of detecting smoke from forest fires(EfficientDet, Faster R-CNN, YOLOV3, and SSD). It turns out that YOLO V3enables the fastest detection (27 frame par seconds), while EfficientDetachieves the highest accuracy (95.7%) (Zheng et al., 2022). To reduce the trainingtime, it should be noted that all models used the transfer learningmethod (Pundir &amp; Raman, 2019; Zhang et al., 2020; Han et al., 2022; Zheng et al., 2022) based on pre-trained models.It appears that a machine vision model based on a Deep Learning modelwould be a reliable way to detect smoke plumes. Using the videoscollected in Guatemala (Flight Lab, n.d.), it will be possible to create acustomized database to train the detection model for the specificdetection of volcano plumes in the same way as the database created byHan to detect ship emissions (Han et al., 2022).Machine Vision for trajectory guidanceThe literature highlights the potential use of machine vision techniquesin drone applications, including autonomous surveillance, search andrescue mapping, aerial refuelling, and industrial facility inspection(Arafat et al., 2023). Within the context of UAVs, machine vision algorithms canbe categorized into three main categories: visual localization andmapping, obstacle avoidance, and visual servoing (Arafat et al., 2023; Al-Kaff et al., 2018).This dissertation topic focuses specifically on state-of-the-art visualservoing techniques. Visual servoing enables the utilization of visualdata acquired by an onboard camera to provide feedback to a control loop(Al-Kaff et al., 2018; Campoy et al., 2010). Therefore, with the aid of this technique, thedetection of smoke plumes can be utilized as an input parameter to acontrol law that guides the BUDDI into the plume.Similar applications can be found in the literature. Singh et al. usedfeature detection in an image to perform object tracking based on aViola-Jones algorithm and blob analysis (Singh &amp; Anvar, 2014). Prabowo et al.devised a visual servoing system, which empowers a fixed-wing drone totrack a target by utilizing the position of a feature in an image as aninput of the control loop. The latter makes it possible to influence thevarious control surfaces of the fixed-wing drone (ailerons, elevator anddrift). The efficiency of the developed algorithms was verified throughthe application of the hardware in the loop simulation (HITS) technique(Prabowo et al., 2015). A purely simulation-based method was used by Aygun et al.to test a nonlinear vision-based missile guidance law. The latterconsists of a visual servoing method used in conjunction with asliding-mode control strategy. The simulation conducted on MATLABdemonstrates that the approach used allows the missile to determine atrajectory enabling it to intercept both stationary and moving targets(Aygun et al., 2014). In the same way, a simulated nonlinear-model predictivecontroller is developed by Cheng et al. to track a moving feature thanksto a multi-copter (Chen et al., 2021). Using computer simulations andexperimental tests on a multirotor, Liang et al. were able to develop animage-based servoing controller based on a perspective projection modeland the implementation of a velocity observer in translation byobserving features presented in the image. The quadcopter and visualinformation dynamics are combined within a nonlinear controller based onthe backstepping technique (Zheng et al., 2016). The literature alludes toanalogous studies where a drone can trace a path by leveraging visualdata, and nonlinear controllers seem to be the more favorable option.The same approach can potentially be employed to design a plumeinterception simulator for BUDDI by adopting a numerical method akin tothat of (Aygun et al., 2014).Impact AssessmentA new tool for monitoring volcanic eruptionsIn the event of the democratization of drone usage in volcanology,scientists could access a multitude of new data, allowing them to studypreviously inaccessible volcanoes to ground-based scientific teams. Forinstance, the summit of Fuego in Guatemala cannot be approached within 1km (Watson, 2017). More systematic data collection through dronescould significantly improve the accuracy of predicting future eruptions,enabling decision-makers to implement adequate protective measures topreserve local populations from the effects of volcanoes. Predictinglava flows or pyroclastic surges would be particularly essential forthis purpose. Given that 1/10th of the world’s population lives in anarea where volcanism can have a negative impact, improved samplingmethods around volcanoes can have a significant societal impact(Brown et al., 2017).If the sampling of ash by drones becomes widespread, scientists can toperform numerous successive samplings during an eruption. These willenable estimating the particle size distribution (PSD) much morefrequently than with satellites. As a result, dispersion models will beupdated much more quickly than with satellites and will become morereliable. The improvement in predictions will allow better management ofthe impact of volcanic ash. With better anticipation, it will bepossible to reduce disruptions to economic activities such asagriculture, tourism, and air traffic. Local authorities and airlinecompanies can take prevention and preparation measures to minimize theeconomic costs associated with ash emissions.However, ethically, authorities and scientists must transparently usedrones regarding the nature of the data collected and their use.Organizations must demonstrate responsibility in using prediction modelsand be aware of the risks if the forecasts prove to be incorrect: anunderestimation of risks could make them responsible for human losses,while overly alarming forecasts could lead to premature evacuations,economic losses, and unnecessary trauma for populations. Therefore,volcanology organizations and decision-makers need to address issuesrelated to volcanological data and the limitations of prediction models.Under these conditions, drones will be used ethically and responsibly toserve and protect communities.Long-term impact of using drones for volcanic monitoringVolcanic eruptions are natural events that can take place over anextended period. The University of Paris I reports that the medianduration of an eruption is 7 weeks (de Paris 1, 2008). If multiple drones areutilized to gather data on the eruption during this period, it may leadto environmental issues arising from their impact. Drones used involcanic monitoring must cover extensive distances to reach thevolcano’s summit, which can cause excessive noise pollution, potentiallydisturbing animal species living in the vicinity of the volcanic area.As a result, it is crucial to consider the impact on surrounding animalspecies before initiating a prolonged monitoring campaign.If drone-based volcanic surveillance technology continues to evolve, itwill no longer be exclusively reserved for scientists to conductsurveys. It could also be utilized by companies offering their aerialservices and provide data to government entities for assessing the risksof a volcanic eruption. However, this category of application remainsvery specific since there are only about fifty volcanic eruptions peryear (Ceurstemont &amp; Ceurstemont, 2021), and these companies will only develop in regions withvolcanic activity.From a legal standpoint, the use of these drones for volcanicsurveillance will mainly be in Beyond Visual Line of Sight (BVLOS) mode,which requires a declaration of operations in most countries to fly adrone in regulated airspace. For instance, in Europe, the European UnionAviation Safety Agency (EASA) regulates BVLOS missions using the SORAmethod, which allows for the declaration of the flight zone to theauthorities and risk assessment (Janik et al., 2021). This type of procedure is quitecomplex to implement and time-consuming. If the feasibility of usingdrones in emergencies is proven, governments will be encouraged toestablish infrastructures to simplify using of drones in BVLOS. Forexample, governments could develop an Unmanned Traffic Management (UTM)system to regulate drone flights in their territories in a mannersimilar like already exists for human-crewed aircraft.ReferencesPetursdottir, G., &amp; Reichardt, U. Super Case Study 3: Eyjafjallajökull eruption in 2010. https://doi.org/10.2760/571085Bolić, T., &amp; Sivčev, Ź. (2011). Eruption of Eyjafjallajökull in Iceland: Experience of European air traffic management. Transportation Research Record, 2214(1), 136–143.Gouhier, M. (2019). Le comportement inattendu des nuages de cendres Volcaniques Révélé. In INSU. CNRS. https://www.insu.cnrs.fr/fr/cnrsinfo/le-comportement-inattendu-des-nuages-de-cendres-volcaniques-reveleStohl, A., Prata, A. J., Eckhardt, S., Clarisse, L., Durant, A., Henne, S., Kristiansen, N. I., Minikin, A., Schumann, U., Seibert, P., Stebel, K., Thomas, H. E., Thorsteinsson, T., Tørseth, K., &amp; Weinzierl, B. (2011). Determination of time- and height-resolved volcanic ash emissions and  their use for quantitative ash dispersion modeling: the 2010 Eyjafjallajökull eruption. Atmospheric Chemistry and Physics, 11(9), 4333–4351. https://doi.org/10.5194/acp-11-4333-2011Prata, F. (1989). Radiative transfer calculations for volcanic ash clouds. Geophysical Research Letters - GEOPHYS RES LETT, 16, 1293–1296. https://doi.org/10.1029/GL016i011p01293Prudente, V. H. R., Martins, V. S., Vieira, D. C., de França e Silva, N. R., Adami, M., &amp; Sanches, I. D. A. (2020). Limitations of cloud cover for optical remote sensing of agricultural areas across South America. Remote Sensing Applications: Society and Environment, 20, 100414. https://doi.org/https://doi.org/10.1016/j.rsase.2020.100414Gordeev, E., &amp; Girina, O. (2014). Volcanoes and their hazard to aviation. Herald of the Russian Academy of Sciences, 84, 134–142. https://doi.org/10.7868/S0869587314020121Watson, M. (2017). Spying on volcanoes. Physics World, 30(7), 40.Schellenberg, B. J. (2020). Long range UAS operations for volcanic monitoring [PhD thesis]. University of Bristol.David, T., Hine, D., Schellenberg, B., Goudarzi, H., Rendall, T., Wood, K., Bolos-Fernandez, J., &amp; Richardson, T. S. (2021). CASCADE Open Aircraft Project: University of Bristol VTOL Drone Development. AIAA Scitech 2021 Forum, 1930.Flight Lab, U. of B. Video recordings of flights around the Fuego volcano in Guatemala.Zhang, F., Qin, W., Liu, Y., Xiao, Z., Liu, J., Wang, Q., &amp; Liu, K. (2020). A Dual-Channel convolution neural network for image smoke detection. Multimedia Tools and Applications, 79, 34587–34603.Frizzi, S., Kaabi, R., Bouchouicha, M., Ginoux, J.-M., Fnaiech, F., &amp; Moreau, E. (2017). Détection de la fumée et du feu par réseau de neurones convolutifs. Conférence Nationale Sur Les Applications Pratiques De l’Intelligence Artificielle.Klemas, V. V. (2015). Coastal and Environmental Remote Sensing from Unmanned Aerial Vehicles: An Overview. Journal of Coastal Research, 31(5), 1260–1267. https://doi.org/10.2112/JCOASTRES-D-15-00005.1Wegener, S., Schoenung, S., Totah, J., Sullivan, D. V., Frank, J., Enomoto, F., Frost, C., &amp; Theodore, C. (2004). UAV Autonomous Operations for Airborne Science Missions. https://doi.org/10.2514/6.2004-6416McGonigle, A. J. S., Aiuppa, A., Giudice, G., Tamburello, G., Hodson, A. J., &amp; Gurrieri, S. (2008). Unmanned aerial vehicle measurements of volcanic carbon dioxide fluxes. Geophysical Research Letters, 35(6).Di Stefano, G., Romeo, G., Mazzini, A., Iarocci, A., Hadi, S., &amp; Pelphrey, S. (2018). The Lusi drone: A multidisciplinary tool to access extreme environments. Marine and Petroleum Geology, 90, 26–37. https://doi.org/https://doi.org/10.1016/j.marpetgeo.2017.07.006Mori, T., Hashimoto, T., Terada, A., Yoshimoto, M., Kazahaya, R., Shinohara, H., &amp; Tanaka, R. (2016). Volcanic plume measurements using a UAV for the 2014 Mt. Ontake eruption. Earth, Planets and Space, 68, 1–18.Montes, G., Letheren, B., Villa, T. F., &amp; Gonzalez, F. (2014). Bio-inspired plume tracking algorithm for UAVs. Proceedings of the 16th Australasian Conference on Robotics and Automation 2014, 1–8.Degruyter, W., &amp; Bonadonna, C. (2012). Improving on mass flow rate estimates of volcanic eruptions. Geophysical Research Letters, 39(16).Woodhouse, M. J., Hogg, A. J., Phillips, J. C., &amp; Sparks, R. S. J. (2013). Interaction between volcanic plumes and wind during the 2010 Eyjafjallajökull eruption, Iceland. Journal of Geophysical Research: Solid Earth, 118(1), 92–109. https://doi.org/https://doi.org/10.1029/2012JB009592Cao, Z., Patra, A., Bursik, M., Pitman, E. B., &amp; Jones, M. (2018). Plume-SPH 1.0: a three-dimensional, dusty-gas volcanic plume model based on smoothed particle hydrodynamics. Geoscientific Model Development, 11(7), 2691–2715.Cerminara, M., Esposti Ongaro, T., &amp; Berselli, L. C. (2016). ASHEE-1.0: a compressible, equilibrium–Eulerian model for volcanic ash plumes. Geoscientific Model Development, 9(2), 697–730.Çelik, T., Özkaramanlı, H., &amp; Demirel, H. (2007). Fire and smoke detection without sensors: Image processing based approach. 2007 15th European Signal Processing Conference, 1794–1798.Qi, X., &amp; Ebert, J. (2009). A computer vision based method for fire detection in color videos. International Journal of Imaging, 2(S09), 22–34.Ho, C.-C. (2009). Machine vision-based real-time early flame and smoke detection. Measurement Science and Technology, 20(4), 045502.Chetverikov, D., Fazekas, S., &amp; Haindl, M. (2011). Dynamic texture as foreground and background. Machine Vision and Applications, 22, 741–750.Park, K.-M., &amp; Bae, C.-O. (2020). Smoke detection in ship engine rooms based on video images. IET Image Processing, 14(6), 1141–1149.Hakim, B., &amp; Brahim, S. (2018). Classification des images avec les réseaux de neurones convolutionnels [PhD thesis]. Université Mouloud Mammeri.Pundir, A. S., &amp; Raman, B. (2019). Dual deep learning model for image based smoke detection. Fire Technology, 55(6), 2419–2442.Zhang, F., Qin, W., Liu, Y., Xiao, Z., Liu, J., Wang, Q., &amp; Liu, K. (2020). A Dual-Channel convolution neural network for image smoke detection. Multimedia Tools and Applications, 79, 34587–34603.Han, Y., Li, G., Qin, Q., Wang, S., &amp; Li, Y. (2022). Research on Machine Vision Detection Method of Ship Sulfur Emission Based on Convolutional Neural Network. Journal of Physics: Conference Series, 2171(1), 012071.Zheng, X., Chen, F., Lou, L., Cheng, P., &amp; Huang, Y. (2022). Real-Time Detection of Full-Scale Forest Fire Smoke Based on Deep Convolution Neural Network. Remote Sensing, 14(3), 536.Arafat, M. Y., Alam, M. M., &amp; Moh, S. (2023). Vision-Based Navigation Techniques for Unmanned Aerial Vehicles: Review and Challenges. Drones, 7(2). https://doi.org/10.3390/drones7020089Al-Kaff, A., Martín, D., García, F., de la Escalera, A., &amp; María Armingol, J. (2018). Survey of computer vision algorithms and applications for unmanned aerial vehicles. Expert Systems with Applications, 92, 447–463. https://doi.org/https://doi.org/10.1016/j.eswa.2017.09.033Campoy, P., Mondragón, I. F., Olivares-Méndez, M. A., &amp; Martı́nez Carol. (2010). Visual servoing for UAVs. Visual Servoing, 181.Singh, G., &amp; Anvar, A. (2014). Investigating feasibility of target detection by visual servoing using UAV for oceanic applications. 2014 13th International Conference on Control Automation Robotics &amp; Vision (ICARCV), 1621–1626. https://doi.org/10.1109/ICARCV.2014.7064558Prabowo, Y. A., Trilaksono, B. R., &amp; Triputra, F. R. (2015). Hardware in-the-loop simulation for visual servoing of fixed wing UAV. 2015 International Conference on Electrical Engineering and Informatics (ICEEI), 247–252. https://doi.org/10.1109/ICEEI.2015.7352505Aygun, M. T., MacKunis, W., &amp; Mehta, S. (2014). Robust Image-based Visual Servo Control of an Uncertain Missile Airframe. IFAC Proceedings Volumes, 47(3), 5085–5090. https://doi.org/https://doi.org/10.3182/20140824-6-ZA-1003.02633Chen, C.-W., Hung, H.-A., Yang, P.-H., &amp; Cheng, T.-H. (2021). Visual servoing of a moving target by an unmanned aerial vehicle. Sensors, 21(17), 5708.Zheng, D., Wang, H., Wang, J., Chen, S., Chen, W., &amp; Liang, X. (2016). Image-based visual servoing of a quadrotor using virtual camera approach. IEEE/ASME Transactions on Mechatronics, 22(2), 972–982.Brown, S. K., Jenkins, S. F., Sparks, R. S. J., Odbert, H., &amp; Auker, M. R. (2017). Volcanic fatalities database: analysis of volcanic threat with distance and victim classification. Journal of Applied Volcanology, 6, 1–20.de Paris 1, U. (2008). Evaluation du risque - Magnitude d’un aléa. In Evaluation du risque - Magnitude d’un aléa. https://e-cours.univ-paris1.fr/modules/uved/risques-naturels/html/2/21/211/2112.html#: :text=Pour%20l’ensemble%20des%20volcans,%C3%A9ruption%20est%20de%207%20semaines.Ceurstemont, S., &amp; Ceurstemont, S. (2021). Unravelling the when, where and how of volcanic eruptions. In Horizon Magazine. https://ec.europa.eu/research-and-innovation/en/horizon-magazine/unravelling-when-where-and-how-volcanic-eruptionsJanik, P., Zawistowski, M., Fellner, R., &amp; Zawistowski, G. (2021). Unmanned Aircraft Systems Risk Assessment Based on SORA for First Responders and Disaster Management. Applied Sciences, 11(12). https://doi.org/10.3390/app11125364"
  },
  
  {
    "title": "Study of the curvature of a fluidic elastomer gripper",
    "url": "/posts/Soft-Gripper/",
    "categories": "Academic Projects, Bristol",
    "tags": "Academic, Studies, Bristol, Soft Robotics, Robotics",
    "date": "2023-06-05 00:00:00 +0200",
    





    
    "snippet": "This article originates from a coursework conducted at the University of Bristol, focusing on the fabrication and investigation of a Pneu-Net soft gripper architecture. The following video provides...",
    "content": "This article originates from a coursework conducted at the University of Bristol, focusing on the fabrication and investigation of a Pneu-Net soft gripper architecture. The following video provides an overview of the operational principles of this type of gripper.  The soft gripper in actionIntroduction : a quick literature review on fluidic elastomer grippersGrippers are essential for robots to interact with and manipulateobjects in their environment. In industrial applications, traditionalgrippers consist of rigid links and joints that deform to grip anobject. However, their structural and kinematic complexity can makecontrol challenging. This report focuses on a less complex alternative:fluidic elastomer grippers. The latter are composed of an elastomer structure with an internalnetwork of fluidic channels. The injection of pressurised fluid insidethese channels can deform the gripper’s structure, and manage itsopening and closing. The soft nature of the gripper allows it to graspfragile objects delicately. This characteristic has been tested inapplications such as coral sampling in the deep sea  (Galloway et al., 2016). Actuatorsof this type can be controlled using various fluids, but the air ispreferred due to its low viscosity. This characteristic allows lowerpressure forces to be exerted within the gripper (Marchese et al., 2015).The most commonly used manufacturing method for fluidic elastomergrippers is castable silicone moulded into 3D printed parts. The mouldallows the generation of silicon sections of different stiffnessesallowing the deformation of the gripper when it is being pressurised. 3Dprinting to produce these moulds offers numerous advantages, such asreduced manufacturing costs, rapid prototyping, and repeatability(Marchese et al., 2015; Mosadegh et al., 2014). The scaffold-removalmethod, in which internal structures are added to the elastomer andlater dissolved, can create complex shapes that cannot be achieved withthe previously mentioned technique (Saggiomo &amp; Velders, 2015).The most commonly used channel architecture in soft robotics is thePneu-Net architecture, which consists of two parts, as shown in Fig.1. The firstpart is designed to be expandable and contains the desired channelstructure and expansion chambers. The second part is reinforced with anon-expandable material, such as fabric or fibres.{#Pneu-Net}Fig. 1 Pneu-Net Architecture - Illustation from (Mosadegh et al., 2014)When pressurised, the first part expands, stretching the walls of thechambers in the expandable section. This deformation causes the entireactuator to flex (Marchese et al., 2015; Ilievski et al., 2011).Different manufacturing methods and architectures were also explored.Wang et al. were able to develop a 3-fingered gripper composed entirelyof 3D-printed parts (Wang &amp; Hirai, 2017). Yang et al. contributed by using thecontraction principle to operate a gripper’s opening and closing. Theirmethod is to apply negative pressure in a solid molded with siliconethat contains cavities. Under the pressure, the cavities retract anddeform the material. The solid is then equipped with extensions that actas fingers. The contraction and relaxation of the material allows themodification of the space between the extensions, thus allowing theopening and closing of the gripper (Yang et al., 2015). The article (Zhou et al., 2022) presents a gripper composed of several prestrained fluid composite actuators. Each actuator uses a restraining layer made of composite to obtain a pre-curved equilibrium shape. The other layers made of elastomers flatten the composite part when pressure is introduced into the actuator. Unlike previously presented technologies, this method has the advantage that the system’s equilibrium position corresponds to the closed gripper position.The literature includes several studies proposing various methods to improve the efficiency and intelligence of fluidic elastomer grippers. For instance, Glick et al. used biomimicry to enhance gripping performance by creating a gripper consisting of two Pneu-Net actuators coated with a gecko-inspired adhesive. This approach enabled the gripper to improve its grasping abilities (Glick et al., 2018). The manufacturing process of grippers also allows for the direct integration of sensors into the gripper structure to make them intelligent. For instance, in (Wang &amp; Hirai, 2016), curvature sensors were integrated into each finger of a gripper to evaluate their curvature during object grasping. Similarly, (Yang et al., 2020) integrated resistive deformation sensors and capacitive proximity sensors into a Pneu-Net gripper to enable it to detect the size of the grasped objects. Yang et al. have successfully developed a 3D printed pneumatic actuator inspired by the kinematics of the human finger. The actuator can be printed in one piece and contains pressure and position sensors integrated at each joint. This innovation opens up possibilities for applications requiring closed-loop control (Yang &amp; Chen, 2017).Gripper FabricationThe gripper design, provided by the University of Bristol, features thePneu-Net architecture, which consists of 8 chambers located on eitherside of the air supply. The gripper’s inextensible layer is mainly madeof J-cloth fabric, while its extensible layer is made of Eco Flex00-30silicone material (Company, 2019). It should be noted that the mechanicalproperties of J-Cloth are anisotropic : it can stretch more easily inone direction than another.Materials providedTo fabricate the gripper, the University of Bristol provides a kit thatcontains the following elements:      Ecoflex 00-30 (Composed of two liquids (A and B) to be mixedtogether to obtain silicone.)        Nitrile rubber gloves, Blue paper (for cleaning)        Mixing sticks and a Mixing pot        A 3D printed mould (Shown in Fig.2)        Syringe, a connector and tubing        J-cloth (A type of absorbent fabric whose initial application isfound in the household domain)              Fig. 2 CAD model of the 3D printed mouldSafety conditionsEven though both components used in the silicone fabrication process arenot skin irritants, as a preventive measure, all handling involvingliquid Ecoflex will be carried out with nitrile gloves.Fabrication processAll the instructions for making the gripper are provided in a video madeby Dr. Alix Partridge. The instructions can be summarized in thefollowing three steps:      Silicone Fabrication Process : To make silicone, the same processwill always be used: mix each EcoFlex component in equal proportionsin a mixing pot, then pour the resulting mixture into a syringe. Byplugging the end of the syringe, and pulling the plunger, it ispossible to create vacuum. This will eliminate air bubbles in theliquid. This process should be repeated at least 10 times.        First step : To start, cut a piece of J-cloth slightly larger thanthe mould surface and place it on a smooth surface, like laminatedcardboard. Ensure that the direction in which the J-tissue canstretch is aligned with the longitudinal axis of the mould. Also besure to position a piece of tube at the designated location in thecentre of the mould. Then make silicone using the method describedabove. Pour the silicone into the mould being careful not tointroduce air bubbles. The remaining silicone can be applied to theJ-Cloth using the supplied mixing sticks. Let the silicone dry for aminimum of 4 hours.        Second step : To start, demold the silicone and peel off theJ-Cloth from the smooth surface. Then, cover the base of the moldedpart with a thin layer of liquid silicone using your finger. Coatthe surface of the J-Cloth that was in contact with theplastic-coated cardboard with silicone as well. Next, place the twocoated layers in contact, taking care to avoid the formation of airbubbles at the joint. After completing this step, it is important toallow the silicone to dry for 4 hours.        Third step : Connect the gripper tube to the syringe using theadapter, pressurize it and check for proper functioning  Experimental designIn the framework of this project, two main experiments are envisioned:the first one focuses on studying the curvature of the gripper as afunction of the injected air volume, and the second one aims todetermine the gripper’s performance in grasping multiple types ofobjects.Curvature StudyHypothesisBy compressing the gripper with different volumes, it is observed thatthe curvature of the gripper appears to approach that of a circular arc.To experimentally test this hypothesis, a measurement method isestablished to record the gripper’s curvature using a camera. A circularfitting method will be used to verify if the curvature of the grippercan be inscribed in a circle. If this hypothesis is validated throughthe experiment, it will also be possible to plot the gripper clampingradius evolution law as a function of the injected air volume.[]{#part:hypothese label=”part:hypothese”}Setting up the test benchFor precise measurements, a test bench is utilized to hold the gripperfixed during compression. This setup ensures a constant fixed positionof the camera and gripper for all tested air volumes. The test bench, asshown in Fig. 3, also includes a graduated ruler that serves asa scale during machine vision analysis. A black background is placed toaid the machine vision procedure, and red pinheads are evenlydistributed along the gripper for subsequent identification of itscurvature (Fig. 3). The machine vision technique employed iselaborated in a following sectionProposed experimental protocolThe following method is used for each volume from 0 to 60 ml with a stepof 5 ml:      The considered volume is injected into the gripper and a paper clipis used to clamp the tube and keep the gripper in a fixed position.        A scene acquisition is performed using a camera, which is asmartphone held on a tripod.        The coordinates of the gripper pinheads are extracted using amachine vision algorithm, which provides pixel coordinates in theimage reference frame.        The circle-fit library (“Circle-Fit - A Circle Fitting Library for Python,” n.d.) is used to determine the parametersof the circle (center coordinates and radius) that best fits all thegripper points. The accuracy and precision of the model areevaluated using the value of the residual error provided by thelibrary. The results obtained are returned in pixels, the scalepresent on the photo allows to convert the found values intocentimeters.        The gripper markers and the fitting circle are displayed on the samegraph.  First, these data will allow us to study the evolution of the residualerror according to the injected air volume, in order to check if thegripper curvature can be approximated by a circle.Description of the machine vision method usedThe method used is based on the elements provided in the document(Qi &amp; Conn, 2023). For each image to be processed, the same method isapplied. The different functions used come from the Python module OpenCVwhich allows image processing (Bradski, 2000). The coordinates of thegripper points are stored in a list and are subsequently used forinterpretations. The steps of the machine vision algorithm are describedin Fig. 3.            Fig. 3 Test Bench made of LegosThe entire code used in this project is available in the dedicatedGithub repository which can be foundhere.Study of the grasping capability of the soft gripperHypothesisAn assumption is made that the gripper is capable of lifting a widerange of differently shaped objects. The following experiment aims totest the gripper’s ability to lift various types of objects.Proposed experimental protocolThe objective of this experiment is to study the configuration andgrasping capability of the gripper on various everyday objects. Theexperimental protocol involves the following steps:      Select an object and measure its mass.        Place the object on a flat surface.        Position the gripper on the object and compress the syringe.        Attempt to lift the object while taking a photograph of theconfiguration.  Results and AnalysisCurvature StudyMachine Vision ResultsThe machine vision method is applied and allows obtaining the gripper’sshape for different volumes as presented in Fig.4. For display convenience, only half ofthe tested volumes are shown.        Fig.4 Gripper gaitsIt is possible to observe that when the volume is zero (the gripper isnot compressed), it is already curved due to the effect of gravity onthe soft material.Circular fitting results and interpretationsUsing the different coordinates of the points, a circular fitting isperformed for each volume value. Fig.5 shows 4 examples of circular fitting.        Fig.5 Circular Fitting²As the quantity of injected air increases, fitting a circle to thecurvature of the gripper becomes more difficult. This is supported bythe residual error curve (Fig.6), which represents the root mean squareerror between data points and the circumference of the regression circle(“Circle-Fit - A Circle Fitting Library for Python,” n.d.). The gripper’s curvature can be approximated by a circular arcfrom 0 to 25 ml, with a residual error below 1 mm. However, the residualerror increases beyond 30 ml, indicating that the curvature cannot beapproximated by a circular arc. Thus, presenting the clamping radius asa function of the gripper is not relevant from 0 to 60 ml. Thehypothesis stated in sectionThe hypothesis part is not validated.        Fig.6 Residual error as a function of injectedvolumeStudy of another parameterIt is not possible to accurately represent the entire curvature of thegripper using a circular arc. However, an appropriate parameter is beingsought to quantify this curvature. Therefore, the concept of the\"corrected clamping radius\" is introduced, which corresponds to theradius of the circle passing through the center and the gripper ends asshown in Fig. 7. This parameter indirectly quantifies thesize of objects that the gripper can grasp. Finding a model to representthe evolution of this parameter is relevant as it could serve as acontrol law in a more intricate system.        Fig.7 Definition of the Corrected clampingradiusThus, the circular fitting method is applied to only 3 points. Given thenumber of points considered, circular fitting is perfect, and for eachvolume value, the corrected clamping radius is derived. The resultinggraph is presented in Figure8.        Fig.8 Evolution of corrected clampingradiusThe experimental points obtained allow a modeling of the correctedclamping radius (CCR) as a function of the volume. A polynomialregression is applied to the data, and it turns out that a 5th orderregression yields the best correlation coefficient without overfitting.The derived equation is shown inFig.8.Study of the grasping capability of the soft gripperBy using the experimental protocol mentioned, thegripper lifted the different objects presented in Fig.9: a cardboard box,a 3D printed piece, a Rubik’s Cube, and a meter.The gripper was found to be able to grasp a range of objects as long astheir weight is less than 120g. Beyond this threshold, the gripperslides and is unable to hold the object. In addition, the shape of thegripper does not match that of the object it grabs: only the two endsare in contact.To improve the performance of the gripper, it could be considered to adda rough surface like sandpaper to ensure better adhesion, similar to thework done by (Glick et al., 2018).        Fig.9 Various tested objectsradiusDiscussionImprovement suggestions on the fabrication methodTubing problemDuring the manufacturing process, a difficulty arose during thedemolding stage, resulting in the tube being held in the mold anddetachment from the silicone part, as shown in Fig.10{reference-type=”ref” reference=”fig:probleme”}. Iattempted to remedy the situation by using glue to reattach the tube,but the physical properties of the silicone hindered the adhesion of theglue, rendering this solution ineffective.        Fig.10 Fabrication problem encounteredTo address the difficulty at hand, the manufacturing process of themolded part had to be restarted with various precautions taken. Abrasivepaper was utilized to sand the outer surface of the tube for bettersilicone adhesion, and the part of the mold that held the tube was alsosanded to make detachment easier. Once the tube was properly positionedin the mold, the tube base was degreased with 90% alcohol. With thesemeasures in place, the molded part of the gripper was successfullymanufactured without any detachment issues.Air leakage issuesDuring the manufacturing process, another issue was observed whichconcerned air leakage during the final gripper testing. These leaks werefound at the junction between the J-Cloth and the molded part. Anadditional step was added to the process to overcome this issue. At theend of the second step of the manufacturing process, a silicone sealant is applied all around themolded part to prevent leakage. The sealant is depicted in yellow inFig. 11.        Fig.11 Addition of a seal to ensure tightness.Suggestion for design improvementThe current gripper could only limit two degrees of freedom during thegrasping experiment due to its two contact points. To improve this, afeasible solution would be to add a new part to the gripper that blocksall three degrees of freedom, enhancing grasping quality and preventingobject movement on all axes. Increasing the contact surface between thegripper and the object by adding a third contact point would alsoimprove grip. To achieve this, we modified the existing gripper’s CADmodel, creating a three-fingered gripper with 120-degree spacing betweeneach finger. The resulting 3D model was then printed (Fig.12).        Fig.12 Gripper design proposalA silicone shortage prevented full fabrication of the designed gripperfor performance testing and validation of previous hypotheses. Toaddress this, an extension of this work could involve repeating thegrasping experiment using the new gripper design and quantifyingperformance differences between the original and new designs.ConclusionThis project allowed for the discovery of theoretical and practicalconcepts related to fluidic elastomer grippers, with a focus on thePneu-Net architecture. The curvature of the gripper during compressioncan only be considered as an arc of a circle in the volume range of 0 to25 milliliters. By introducing the parameter of corrected clampingradius, it was possible to establish a polynomial relationship betweenthis parameter and the injected air volume. It should be noted that thelaw modeled is specific to the gripper fabricated in this report.Indeed, variations in fabrication can modify the curvature behavior ofthe gripper. The gripping capacity of the gripper was also investigated,demonstrating its ability to grasp various objects of different shapesand masses. Finally, design and manufacturing improvements were proposedfor the gripper.Galloway, K. C., Becker, K. P., Phillips, B., Kirby, J., Licht, S., Tchernov, D., Wood, R. J., &amp; Gruber, D. F. (2016). Soft robotic grippers for biological sampling on deep reefs. Soft Robotics, 3(1), 23–33.Marchese, A. D., Katzschmann, R. K., &amp; Rus, D. (2015). A recipe for soft fluidic elastomer robots. Soft Robotics, 2(1), 7–25.Mosadegh, B., Polygerinos, P., Keplinger, C., Wennstedt, S., Shepherd, R. F., Gupta, U., Shim, J., Bertoldi, K., Walsh, C. J., &amp; Whitesides, G. M. (2014). Pneumatic networks for soft robotics that actuate rapidly. Advanced Functional Materials, 24(15), 2163–2170.Saggiomo, V., &amp; Velders, A. H. (2015). Simple 3D printed scaffold-removal method for the fabrication of intricate microfluidic devices. Advanced Science, 2(9), 1500125.Ilievski, F., Mazzeo, A. D., Shepherd, R. F., Chen, X., &amp; Whitesides, G. M. (2011). Soft robotics for chemists. Angewandte Chemie, 123(8), 1930–1935.Wang, Z., &amp; Hirai, S. (2017). Soft Gripper Dynamics Using a Line-Segment Model With an Optimization-Based Parameter Identification Method. IEEE Robotics and Automation Letters, 2(2), 624–631. https://doi.org/10.1109/LRA.2017.2650149Yang, D., Mosadegh, B., Ainla, A., Lee, B., Khashai, F., Suo, Z., Bertoldi, K., &amp; Whitesides, G. M. (2015). Buckling of elastomeric beams enables actuation of soft machines. Advanced Materials, 27(41), 6323–6327.Zhou, Y., Headings, L. M., &amp; Dapino, M. J. (2022). Modeling of soft robotic grippers integrated with fluidic prestressed composite actuators. Journal of Mechanisms and Robotics, 14(3).Glick, P., Suresh, S. A., Ruffatto, D., Cutkosky, M., Tolley, M. T., &amp; Parness, A. (2018). A soft robotic gripper with gecko-inspired adhesive. IEEE Robotics and Automation Letters, 3(2), 903–910.Wang, Z., &amp; Hirai, S. (2016). A 3D printed soft gripper integrated with curvature sensor for studying soft grasping. 2016 IEEE/SICE International Symposium on System Integration (SII), 629–633. https://doi.org/10.1109/SII.2016.7844069Yang, T. H., Shintake, J., Kanno, R., Kao, C. R., &amp; Mizuno, J. (2020). Low-Cost Sensor-Rich Fluidic Elastomer Actuators Embedded with Paper Electronics. Advanced Intelligent Systems, 2(8), 2000025.Yang, Y., &amp; Chen, Y. (2017). Innovative design of embedded pressure and position sensors for soft actuators. IEEE Robotics and Automation Letters, 3(2), 656–663.Company, C. P. T. (2019). EcoFlex00-30 Skin Safe Certification.Circle-fit - A Circle Fitting Library for Python. In PyPI. https://pypi.org/project/circle-fit/Qi, Q., &amp; Conn, A. (2023). Basics of Image Processing in Soft Robotics Module. Soft Robotics 2022, 1–8.Bradski, G. (2000). The OpenCV Library. Dr. Dobb’s Journal of Software Tools."
  },
  
  {
    "title": "MSc Robotics University of Brisol",
    "url": "/posts/MSc-Robotics-University-of-Bristol/",
    "categories": "Academic training",
    "tags": "Academic, Studies, Bristol, Robotics",
    "date": "2023-06-05 00:00:00 +0200",
    





    
    "snippet": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France.",
    "content": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France."
  },
  
  {
    "title": "Arts et Métiers Paristech - General Presentation",
    "url": "/posts/Arts-et-M%C3%A9tiers-ParisTech/",
    "categories": "Academic training",
    "tags": "Academic, Studies, Châlons-en-Champagne, GadzArts",
    "date": "2023-06-05 00:00:00 +0200",
    





    
    "snippet": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France.",
    "content": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France."
  },
  
  {
    "title": "Academic Background - General Presentation",
    "url": "/posts/Academic-Background/",
    "categories": "Academic training",
    "tags": "Academic, Studies",
    "date": "2023-06-05 00:00:00 +0200",
    





    
    "snippet": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France.",
    "content": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France."
  },
  
  {
    "title": "Classe préparatoire aux grandes écoles - General presentation",
    "url": "/posts/CPGE-PSI/",
    "categories": "Academic training",
    "tags": "Academic, Studies, Reims, CPGE, PSI",
    "date": "2023-02-05 00:00:00 +0200",
    





    
    "snippet": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France.",
    "content": "The following diagram summarizes the structure of my academic journey, which was mostly completed in France."
  }
  
]

